\chapter{Theoretische Grundlagen}

Laut [ARVR] lässt sich eine Augmentierte Realität mit den nachfolgenden fünf Schritten realisieren:

\begin{enumerate}
    \item Videoaufnahme
    \item Tracking
    \item Registrierung
    \item Darstellung
    \item Ausgabe
\end{enumerate}

In diesem Kapitel werden die theoretischen Grundlagen der einzelnen Schritte erläutert. 

\section{Dreidimensionale Grafik}

Um Augmented Reality und verwandte Technologien zu verstehen, ist ein grundlegendes Verständnis der mathematischen und geometrischen Prinzipien erforderlich, die der Darstellung und Manipulation dreidimensionaler Objekte zugrunde liegen. Diese Prinzipien bilden die Basis für die Erzeugung und Interaktion mit virtuellen Szenen und Objekten. Im Folgenden werden die Konzepte zur Beschreibung, Transformation und Darstellung dreidimensionaler Objekte erläutert.

\subsection{Dreidimensionale Objekte}

In der Computergrafik werden dreidimensionale Objekte, auch als Modelle bezeichnet, durch geometrische und relationale Informationen beschrieben. Diese Objekte bestehen in der Regel aus Polygonen, die durch ihre Eckpunkte (Vertices) definiert sind. Ein Polygon ist eine geschlossene Fläche, die durch das Verbinden der Vertices mit geraden Linien entsteht. Jedem Vertex können zusätzliche Attribute wie Farbe, Texturkoordinaten oder Normalenvektoren zugewiesen werden.

Das einfachste Polygon ist ein Dreieck, das nur drei Eckpunkte benötigt. Dreiecke sind planar und konvex, was sie ideal für Berechnungen in der Computergrafik macht, wie beispielsweise bei der Beleuchtung oder Kollisionserkennung. Obwohl komplexere Polygone existieren, werden diese oft in Dreiecke zerlegt, da sie von Grafikpipelines effizienter verarbeitet werden können. Solche Modelle bestehen dann aus Dreiecksnetzen (Meshes), die als Arrays von Vertices und Indizes gespeichert werden.

\subsection{Welt- und lokale Koordinaten}

Virtuelle 3D-Szenen und Objekte werden in einem kartesischen Koordinatensystem beschrieben. Dieses Koordinatensystem wird üblicherweise als Weltkoordinatensystem bezeichnet, in dem die \(x\)-, \(y\)- und \(z\)-Achsen die drei Dimensionen repräsentieren. In einem rechtshändigen Koordinatensystem zeigt die \(x\)-Achse nach rechts, die \(y\)-Achse nach oben und die \(z\)-Achse nach vorne.

Jedes 3D-Objekt hat zusätzlich ein eigenes lokales Koordinatensystem. Die Vertices eines Objekts werden in diesem lokalen Koordinatensystem definiert. Um ein Objekt in der Szene zu platzieren, werden seine lokalen Koordinaten mithilfe von Transformationen in die Weltkoordinaten transformiert.

\subsection{Transformationen}

Transformationen werden verwendet, um Objekte in einer 3D-Szene zu verschieben, zu drehen oder zu skalieren. Diese Vorgänge werden durch Transformationsmatrizen beschrieben, die einheitlich auf alle Vertices eines Objekts angewendet werden. Solche Matrizen haben typischerweise die Größe \(4 \times 4\) und kombinieren Translation, Rotation und Skalierung.

\subsubsection{Translation (Verschiebung)}

Die Translation verschiebt ein Objekt entlang der \(x\)-, \(y\)- oder \(z\)-Achse. Die Transformationsmatrix für eine Translation ist wie folgt definiert:

\begin{equation}
\begin{bmatrix}
x' \\ y' \\ z' \\ 1
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 0 & t_x \\
0 & 1 & 0 & t_y \\
0 & 0 & 1 & t_z \\
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
x \\ y \\ z \\ 1
\end{bmatrix}
\end{equation}

Hierbei verschieben die Parameter \(t_x\), \(t_y\) und \(t_z\) das Objekt entlang der jeweiligen Achsen.

\subsubsection{Rotation (Drehung)}

Die Rotation eines Objekts erfolgt um eine der drei Achsen (\(x\), \(y\) oder \(z\)). Die Rotationsmatrizen sind für jede Achse wie folgt definiert:

\paragraph{Rotation um die \(x\)-Achse:}
\begin{equation}
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & \cos(\alpha) & -\sin(\alpha) & 0 \\
0 & \sin(\alpha) & \cos(\alpha) & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\end{equation}

\paragraph{Rotation um die \(y\)-Achse:}
\begin{equation}
\begin{bmatrix}
\cos(\alpha) & 0 & \sin(\alpha) & 0 \\
0 & 1 & 0 & 0 \\
-\sin(\alpha) & 0 & \cos(\alpha) & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\end{equation}

\paragraph{Rotation um die \(z\)-Achse:}
\begin{equation}
\begin{bmatrix}
\cos(\alpha) & -\sin(\alpha) & 0 & 0 \\
\sin(\alpha) & \cos(\alpha) & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\end{equation}

Die Reihenfolge, in der Rotationen um verschiedene Achsen durchgeführt werden, ist entscheidend, da sie die endgültige Orientierung beeinflusst. Diese Reihenfolge wird durch Euler-Winkel beschrieben.

\subsubsection{Skalierung (Größenanpassung)}

Die Skalierung verändert die Größe eines Objekts proportional entlang der \(x\)-, \(y\)- und \(z\)-Achsen. Die Skalierungsmatrix ist wie folgt definiert:

\begin{equation}
\begin{bmatrix}
x' \\ y' \\ z' \\ 1
\end{bmatrix}
=
\begin{bmatrix}
s_x & 0 & 0 & 0 \\
0 & s_y & 0 & 0 \\
0 & 0 & s_z & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
x \\ y \\ z \\ 1
\end{bmatrix}
\end{equation}

Hierbei sind \(s_x\), \(s_y\) und \(s_z\) die Skalierungsfaktoren entlang der jeweiligen Achsen.

\subsubsection{Anwendung der Transformationen}

Die lokalen Koordinaten eines Objekts können mithilfe einer Transformationsmatrix in Weltkoordinaten umgerechnet werden:

\begin{equation}
P_{world} = T_{object} \cdot P_{object}
\end{equation}

Hierbei ist \(T_{object}\) die Transformationsmatrix des Objekts, und \(P_{object}\) sind die lokalen Koordinaten. Das Ergebnis \(P_{world}\) beschreibt die Position des Objekts im Weltkoordinatensystem.

\section{Kalibrierung}

In Augmented-Reality-Anwendungen ist eine präzise Überlagerung digitaler Informationen auf die reale Welt essenziell, um ein realistisches und funktionales Benutzererlebnis zu schaffen. Die Kamerakalibrierung spielt hierbei eine entscheidende Rolle, da sie sicherstellt, dass virtuelle Objekte korrekt positioniert, skaliert und perspektivisch angepasst in die physische Umgebung integriert werden.

Die Kalibrierung umfasst die Bestimmung der \textbf{intrinsischen} und \textbf{extrinsischen} Parameter der Kamera:

\begin{itemize}
    \item \textbf{Intrinsische Parameter} beschreiben die optischen Eigenschaften der Kamera, wie Brennweiten, Verzerrungen und die Position des optischen Zentrums.
    \item \textbf{Extrinsische Parameter} legen die Position und Orientierung der Kamera im Raum fest, was für die korrekte Transformation zwischen Welt- und Bildkoordinaten erforderlich ist.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[ width=.5\textwidth ]{calibration-cameramodel-coords}
    \caption{Modell für die Kamerakalibrierung\label{fig:Kalibrierung}}\par
\end{figure}

Mathematisch wird die Transformation durch die Gleichung \( x = PX \) beschrieben, wobei:

\begin{itemize}
    \item \( x \) die Bildkoordinaten in Pixeln sind,
    \item \( P \) die Projektionsmatrix ist und
    \item \( X \) die Weltkoordinaten eines Objekts darstellt.
\end{itemize}

Die Projektionsmatrix \( P \) setzt sich aus der intrinsischen Matrix \( K \) sowie der Rotationsmatrix \( R \) und dem Translationsvektor \( t \) zusammen:
\[
P = K[R|t]
\]

Die intrinsische Matrix \( K \) wird folgendermaßen definiert:
\[
K = 
\begin{bmatrix}
f_x & s & c_x \\
0 & f_y & c_y \\
0 & 0 & 1
\end{bmatrix}
\]
\begin{itemize}
    \item \( f_x \) und \( f_y \): Brennweiten der Kamera in Pixeln, bezogen auf die horizontalen und vertikalen Achsen.
    \item \( c_x \) und \( c_y \): Koordinaten des optischen Zentrums in Pixeln.
    \item \( s \): Skew-Faktor, der berücksichtigt, ob die Kameraachsen orthogonal sind.
\end{itemize}

Die Transformation von Weltkoordinaten in Bildkoordinaten wird schließlich durch die Matrix-Multiplikation berechnet:
\[
\begin{bmatrix}
x \\ y \\ 1
\end{bmatrix}
= 
\begin{bmatrix}
f_x & 0 & c_x \\
0 & f_y & c_y \\
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
r_{11} & r_{12} & r_{13} & t_1 \\
r_{21} & r_{22} & r_{23} & t_2 \\
r_{31} & r_{32} & r_{33} & t_3
\end{bmatrix}
\begin{bmatrix}
X \\ Y \\ Z \\ 1
\end{bmatrix}
\]

Zur Kalibrierung werden in der Praxis oft visuelle Marker, wie Schachbrettmuster oder Würfel mit bekannten Geometrien, verwendet. Die Kamera erfasst diese Marker, und mithilfe einer Optimierung, z. B. durch Minimierung eines Fehlermaßes (Loss-Funktion), werden die Kameraparameter berechnet.

Um die Lens-Distortion zu korrigieren, werden oft Polynommodelle, wie das Brown-Conrady-Modell, verwendet. Dieses Modell beschreibt die radiale und tangentialen Verzerrungen der Linse und wird durch die Koeffizienten \( k_1, k_2, k_3 \) und \( p_1, p_2 \) definiert.

Moderne AR-Plattformen wie ARKit oder ARCore integrieren den Kalibrierungsprozess automatisch. ARKit speichert intrinsische Parameter in der Klasse \texttt{AVCameraCalibrationData} und berechnet die extrinsischen Parameter durch die Inertial Measurement Unit (IMU) des Geräts, welche Positions- und Orientierungsdaten liefert.

\section{Tracking-Verfahren}

Unter Tracking versteht man "alle Bearbeitungsschritte, die der gleichzeitigen Verfolgung von (bewegten) Objekten dienen." (Wikipedia) Im Kontext augmentierter Realitäten wird das Tracking verwendet, um die Position und Orientierung von Objekten bzw. Punkten im Bezugs des Koordinatensystems der Kamera zu bestimmen. Mithilfe dieser Daten können virtuelle Objekte korrekt in die reale Welt integriert werden. Es gibt verschiedene Tracking-Verfahren, die auf unterschiedlichen Prinzipien basieren. Im Folgenden werden die wichtigsten Tracking-Methoden für mobile Augmented-Reality-Anwendungen erläutert.

\subsection{Merkmalsbasiertes Tracking}

Beim merkmalsbasierten Tracking wird der Video-Stream der Kamera analysiert, um visuelle Merkmale (Feature-Punkte) in der Umgebung zu erkennen und zu verfolgen. Hierbei hängt die Qualität des Trackings von der Anzahl und Qualität der erkannten Merkmale ab. Umso kontrastreicher und einzigartiger die Merkmale sind, desto robuster und präziser ist das Tracking. 

Zur Erkennung der Feature-Points werden Bildvearbeitungsalgorithmen aus der Computer Vision eingesetzt. Eine weit verbreiteter Algorithmus ist ORB (Oriented FAST and Rotated BRIEF). Dieser baut auf den Algorithmen FAST (Features from Accelerated Segment Test) und BRIEF (Binary Robust Independent Elementary Features) auf und ist besonders effizient und robust. ORB nutzt den FAST-Algorithmus, um Punkte zu finden, die eine starke Helligkeitsänderung aufweisen, und berechnet dann binäre Deskriptoren, um die Merkmale zu beschreiben. Als Erweiterung löscht ORB redundante bzw. unbedeutende Merkmale, die mithilfe einer Harris-Corner-Detektion gefunden wurden. Zuletzt wird eine Rotation des Patches basierend auf der vorher berechneten Orientierung durch, bevor die BRIEF-Berechnung stattfindet. Dadurch bleibt der Deskriptor rotationsinvariant.

Merkmalsbasiertes Tracking ist eine sehr flexible Art des Trackings, da es keine speziellen Marker oder Strukturen in der Umgebung erfordert. Es kann in verschiedenen Szenarien eingesetzt werden, von Innenräumen bis hin zu Außenbereichen. Allerdings ist es anfällig für schlechte Lichtverhältnisse, sich schnell ändernde Umgebungen und sich wiederholenden Strukturen. Als Grundlage der Scene Reconstruction und SLAM (Simultaneous Localization and Mapping) ist es jedoch ein wichtiger Bestandteil vieler AR-Systeme.

\subsection{Markerbasiertes Tracking}

Beim markerbasierten Tracking werden Feature-Punkte in der echten Welt platziert und anschließend von der Kamera erkannt. In der Abbildung \ref{fig:MarkerbasiertesTracking} ist ein Beispiel für ein markerbasiertes Tracking dargestellt. 

Hier wird mithilfe der intrinsichen und extrinsischen Parameter der Kamera (siehe Kapitel 4.2) die Position und Orientierung des Markers im Raum bestimmt. Dabei nimmt man sich die bekannten Dimensionen des Markers zu Nutze, um eine Transformationsmatrix \(T_cm\) zu berechnen, mithilfe derer man die Koordinaten des Markers im lokalen Koordinatensystem ins Kamerakoordinatensystem transformieren kann. Die Bestimmung der Transformation \(T_cm\) erfolgt funktioniert wie folgt:

Es soll gelten:

\[ v_c = T_cm * v_m \]

Nun gilt für einen Bildpixel \(v_s\) unter Anwendung der intrinsischen Kameraparameter (siehe Kapitel 4.2):

\[ v_s = K * v_c \]

Daraus folgt:

\[ v_s = K * T_cm * v_m \]

Da \(v_s\) und \(v_m\) bekannt sind (die Pixelkoordinaten des Markers und die bekannten Dimensionen des Markers), kann die Transformation \(T_cm\) berechnet werden.

Das markerbasierte Tracking ist besonders robust und präzise, da die Position und Orientierung des Markers bekannt sind. Allerdings ist es auch aufwendiger, da die Marker manuell platziert und kalibriert werden müssen. Es wird häufig in Anwendungen eingesetzt, bei denen die Position des darzustellenden Objekts genau bekannt ist. Auch in der Filmproduktion wird markerbasiertes Tracking oft für Motion-Caputre-Verfahren verwendet.

\subsection{Markerloses Tracking}

Beim markerlosen Tracking wird auf in der realen Welt platzierte Features wie beim markerbasierten Tracking verzichtet. Stattdessen werden visuelle Merkmale, wie Ecken, Kanten oder Texturpunkte, in der Umgebung erkannt und verfolgt. Diese Merkmale dienen als Referenzpunkte für die Berechnung der Kameraposition und -ausrichtung. Zur Hilfe werden dabei bei mobilen Smartgeräten oft Sensoren wie Gyroskope und Beschleunigungssensoren verwendet, um die Bewegung des Geräts zu erfassen.

Ein gängiger Ansatz für markerloses Tracking ist die Verwendung von SLAM (Simultaneous Localization and Mapping). SLAM-Algorithmen erstellen eine Karte der Umgebung und bestimmen gleichzeitig die Position des Geräts innerhalb dieser Karte. Dies ermöglicht eine präzise Verfolgung der Bewegung und Position des Geräts in Echtzeit.

Ein weiterer Ansatz ist die Verwendung von Feature-Tracking-Algorithmen wie KLT (Kanade-Lucas-Tomasi) oder ORB (Oriented FAST and Rotated BRIEF). Diese Algorithmen erkennen und verfolgen visuelle Merkmale in aufeinanderfolgenden Kamerabildern, um die Bewegung des Geräts zu berechnen.

Moderne AR-Plattformen wie ARKit und ARCore nutzen eine Kombination aus visuellen und inertialen Tracking-Methoden, um eine robuste und genaue Verfolgung zu gewährleisten. Diese Plattformen integrieren fortschrittliche Algorithmen und nutzen die Hardware-Sensoren der Geräte, um eine nahtlose AR-Erfahrung zu bieten.


